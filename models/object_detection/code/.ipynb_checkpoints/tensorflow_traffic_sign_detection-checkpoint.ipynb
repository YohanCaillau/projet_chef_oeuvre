{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uQCnYPVDrsgx"
   },
   "source": [
    "# Training a Raspberry Pi to Detect Traffic Signs and People in Real Time\n",
    "\n",
    "This is tutorial is based on Chengwei's excellent Tutorial and Colab Notebook on [\"How to train an object detection model easy for free\"](https://www.dlology.com/blog/how-to-train-an-object-detection-model-easy-for-free/).   My twist on his tutorial is that I need to run my model on a Raspberry Pi with live video feed.  As the Raspberry Pi is fairly limited on CPU power and can only run object detection at 1-2 FPS (frames/sec), I have purchased the newly release $75 Google's [EdgeTPU USB Accelarator](https://coral.withgoogle.com/products/accelerator), which can detect objects at 12 FPS, which is sufficient for real time work.  After doing the transfer learning from one of the object detection models using our own images, last few steps of the colab deals with how to convert a trained model to a model file that can be consumed by an Edge TPU, namely, the final `mymodel_quantized_edgetpu.tflite` file.  \n",
    "\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1000/1*_jABdMfUVcyPdi5b3zlfVg.jpeg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T01:24:16.852542Z",
     "start_time": "2021-03-14T01:24:16.849542Z"
    },
    "id": "bSW8d-a8LaIk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T01:24:19.558542Z",
     "start_time": "2021-03-14T01:24:17.735541Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pRcO9PxeLqCT",
    "outputId": "df3c8826-0451-43fd-e75a-1c9433e7ed92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rw-YqZHUKv-Y"
   },
   "source": [
    "# Section 3: Set up Training Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bI8__uNS8-ns"
   },
   "source": [
    "## Install required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:15:55.465525Z",
     "start_time": "2021-03-14T02:15:50.307444Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecpHEnka8Kix",
    "outputId": "628841ae-71b4-4b3b-c932-dd2101cabee8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\Desktop\\object_detection\\content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'models' already exists and is not an empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tf_slim in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (1.1.0)\n",
      "Requirement already satisfied: absl-py>=0.2.2 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from tf_slim) (0.12.0)\n",
      "Requirement already satisfied: six in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from absl-py>=0.2.2->tf_slim) (1.15.0)\n",
      "Requirement already satisfied: lvis in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (0.5.3)\n",
      "Requirement already satisfied: Cython>=0.29.12 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (0.29.22)\n",
      "Requirement already satisfied: matplotlib>=3.1.1 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (3.3.4)\n",
      "Requirement already satisfied: cycler>=0.10.0 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.18.2 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (1.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.4.0 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (2.4.7)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (1.15.0)\n",
      "Requirement already satisfied: kiwisolver>=1.1.0 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (1.3.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.0.25 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from lvis) (4.5.1.48)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\utilisateur\\anaconda3\\envs\\train_traffic-detection\\lib\\site-packages (from matplotlib>=3.1.1->lvis) (8.1.2)\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/utilisateur/Desktop/object_detection/content\n",
    "!git clone --quiet https://github.com/tensorflow/models.git\n",
    "\n",
    "# !apt-get install -qq protobuf-compiler python-pil python-lxml python-tk\n",
    "\n",
    "!pip install -q Cython contextlib2 pillow lxml matplotlib\n",
    "\n",
    "!pip install -q pycocotools\n",
    "\n",
    "!pip install tf_slim\n",
    "\n",
    "!pip install lvis\n",
    "\n",
    "# !pip install tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:15:58.643520Z",
     "start_time": "2021-03-14T02:15:58.641517Z"
    },
    "id": "tgd-fzAIkZlV"
   },
   "outputs": [],
   "source": [
    "test_record_fname = 'C:/Users/utilisateur/Desktop/object_detection/data/annotations/test.record'\n",
    "train_record_fname = 'C:/Users/utilisateur/Desktop/object_detection/data/annotations/train.record'\n",
    "label_map_pbtxt_fname = 'C:/Users/utilisateur/Desktop/object_detection/data/annotations/label_map.pbtxt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhzxsJb3dpWq"
   },
   "source": [
    "# Section 2: Configs and Hyperparameters\n",
    "\n",
    "Support a variety of models, you can find more pretrained model from [Tensorflow detection model zoo: COCO-trained models](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models), as well as their pipline config files in [object_detection/samples/configs/](https://github.com/tensorflow/models/tree/master/research/object_detection/samples/configs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iCNYAaC7w6N8"
   },
   "source": [
    "## Download base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:01.502789Z",
     "start_time": "2021-03-14T02:16:01.497785Z"
    },
    "id": "gnNXNQCjdniL"
   },
   "outputs": [],
   "source": [
    "# If you forked the repository, you can replace the link.\n",
    "repo_url = 'https://github.com/dctian/DeepPiCar'\n",
    "\n",
    "# Number of training steps.\n",
    "num_steps = 1000  # 200000\n",
    "#num_steps = 100  # 200000\n",
    "\n",
    "# Number of evaluation steps.\n",
    "num_eval_steps = 50\n",
    "\n",
    "\n",
    "# model configs are from Model Zoo github: \n",
    "# https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models\n",
    "MODELS_CONFIG = {\n",
    "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\n",
    "    'ssd_mobilenet_v1_quantized': {\n",
    "        'model_name': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18',\n",
    "        'pipeline_file': 'ssd_mobilenet_v1_quantized_300x300_coco14_sync.config',\n",
    "        'batch_size': 12\n",
    "    },    \n",
    "    #http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\n",
    "    'ssd_mobilenet_v2_quantized': {\n",
    "        'model_name': 'ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03',\n",
    "        'pipeline_file': 'ssd_mobilenet_v2_quantized_300x300_coco.config',\n",
    "        'batch_size': 12\n",
    "    }\n",
    "}\n",
    "\n",
    "# Pick the model you want to use\n",
    "# Select a model in `MODELS_CONFIG`.\n",
    "# Note: for Edge TPU, you have to:\n",
    "# 1) start with a pretrained model from model zoo, such as above 4\n",
    "# 2) Must be a quantized model, which reduces the model size significantly\n",
    "selected_model = 'ssd_mobilenet_v2_quantized'\n",
    "\n",
    "# Name of the object detection model to use.\n",
    "MODEL = MODELS_CONFIG[selected_model]['model_name']\n",
    "\n",
    "# Name of the pipline file in tensorflow object detection API.\n",
    "pipeline_file = MODELS_CONFIG[selected_model]['pipeline_file']\n",
    "\n",
    "# Training batch size fits in Colabe's Tesla K80 GPU memory for selected model.\n",
    "batch_size = MODELS_CONFIG[selected_model]['batch_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:19.277287Z",
     "start_time": "2021-03-14T02:16:02.152785Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "orDCj6ihgUMR",
    "outputId": "7f5fc8a2-7082-43f3-8bbc-36eeb451930c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utilisateur\\Desktop\\object_detection\\content\\models\\research\n"
     ]
    }
   ],
   "source": [
    "%cd C:/Users/utilisateur/Desktop/object_detection/content/models/research\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import urllib.request\n",
    "import tarfile\n",
    "MODEL_FILE = MODEL + '.tar.gz'\n",
    "DOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n",
    "DEST_DIR = 'C:/Users/utilisateur/Desktop/object_detection/content/models/research/pretrained_model'\n",
    "\n",
    "if not (os.path.exists(MODEL_FILE)):\n",
    "    urllib.request.urlretrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\n",
    "\n",
    "tar = tarfile.open(MODEL_FILE)\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "\n",
    "os.remove(MODEL_FILE)\n",
    "if (os.path.exists(DEST_DIR)):\n",
    "    shutil.rmtree(DEST_DIR)\n",
    "os.rename(MODEL, DEST_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:20.666291Z",
     "start_time": "2021-03-14T02:16:20.655290Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "UHnxlfRznPP3",
    "outputId": "3bab6af5-87be-4973-d465-a88b99da3573"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/utilisateur/Desktop/object_detection/content/models/research/pretrained_model\\\\model.ckpt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fine_tune_checkpoint = os.path.join(DEST_DIR, \"model.ckpt\")\n",
    "fine_tune_checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYW8J5JoLP4I"
   },
   "source": [
    "# Section 4: Transfer Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvwtHlLOeRJD"
   },
   "source": [
    "## Configuring a Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:24.222847Z",
     "start_time": "2021-03-14T02:16:24.218842Z"
    },
    "id": "dIhw7IdpLuiU"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "pipeline_fname = os.path.join('C:/Users/utilisateur/Desktop/object_detection/content/models/research/object_detection/samples/configs/', pipeline_file)\n",
    "\n",
    "assert os.path.isfile(pipeline_fname), '`{}` not exist'.format(pipeline_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:25.037843Z",
     "start_time": "2021-03-14T02:16:25.034841Z"
    },
    "id": "fG1nCNpUXcRU"
   },
   "outputs": [],
   "source": [
    "def get_num_classes(pbtxt_fname):\n",
    "    from object_detection.utils import label_map_util\n",
    "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
    "    categories = label_map_util.convert_label_map_to_categories(\n",
    "        label_map, max_num_classes=90, use_display_name=True)\n",
    "    category_index = label_map_util.create_category_index(categories)\n",
    "    return len(category_index.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:30.151859Z",
     "start_time": "2021-03-14T02:16:27.400859Z"
    },
    "id": "YjtCbLF2i0wI"
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "bad escape \\m at position 109",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\train_traffic-detection\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse_template\u001b[1;34m(source, pattern)\u001b[0m\n\u001b[0;32m   1014\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1015\u001b[1;33m                     \u001b[0mthis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mESCAPES\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1016\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: '\\\\m'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-320b3028ec2f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# fine_tune_checkpoint: downloaded pre-trained model checkpoint path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     s = re.sub('fine_tune_checkpoint: \".*?\"',\n\u001b[1;32m---> 21\u001b[1;33m                'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# tfrecord files train and test, we created earlier with our training/test sets\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train_traffic-detection\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train_traffic-detection\\lib\\re.py\u001b[0m in \u001b[0;36m_subx\u001b[1;34m(pattern, template)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_subx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;31m# internal: Pattern.sub/subn implementation helper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 311\u001b[1;33m     \u001b[0mtemplate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;31m# literal replacement\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train_traffic-detection\\lib\\re.py\u001b[0m in \u001b[0;36m_compile_repl\u001b[1;34m(repl, pattern)\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_compile_repl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[1;31m# internal: compile replacement pattern\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse_template\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemplate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\train_traffic-detection\\lib\\sre_parse.py\u001b[0m in \u001b[0;36mparse_template\u001b[1;34m(source, pattern)\u001b[0m\n\u001b[0;32m   1016\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1017\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mASCIILETTERS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1018\u001b[1;33m                         \u001b[1;32mraise\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bad escape %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1019\u001b[0m                 \u001b[0mlappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1020\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: bad escape \\m at position 109"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# training pipeline file defines:\n",
    "# - pretrain model path\n",
    "# - the train/test sets\n",
    "# - ID to Label mapping and number of classes\n",
    "# - training batch size\n",
    "# - epochs to trains\n",
    "# - learning rate\n",
    "# - etc\n",
    "\n",
    "# note we just need to use a sample one, and make edits to it.\n",
    "\n",
    "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
    "with open(pipeline_fname) as f:\n",
    "    s = f.read()\n",
    "with open(pipeline_fname, 'w') as f:\n",
    "    \n",
    "    # fine_tune_checkpoint: downloaded pre-trained model checkpoint path\n",
    "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
    "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
    "    \n",
    "    # tfrecord files train and test, we created earlier with our training/test sets\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(train.record)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
    "    s = re.sub(\n",
    "        '(input_path: \".*?)(val.record)(.*?\")', 'input_path: \"{}\"'.format(test_record_fname), s)\n",
    "\n",
    "    # label_map_path: ID to label file\n",
    "    s = re.sub(\n",
    "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
    "\n",
    "    # Set training batch_size.\n",
    "    s = re.sub('batch_size: [0-9]+',\n",
    "               'batch_size: {}'.format(batch_size), s)\n",
    "\n",
    "    # Set training steps, num_steps (Number of epochs to train)\n",
    "    s = re.sub('num_steps: [0-9]+',\n",
    "               'num_steps: {}'.format(num_steps), s)\n",
    "    \n",
    "    # Set number of classes num_classes.\n",
    "    s = re.sub('num_classes: [0-9]+',\n",
    "               'num_classes: {}'.format(num_classes), s)\n",
    "    f.write(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:30.944851Z",
     "start_time": "2021-03-14T02:16:30.892841Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0IH96bbydOWn",
    "outputId": "f66f3466-f360-40b2-d7ee-a55c07cfc4ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item {\n",
      "    id: 1\n",
      "    name: '30 speed sign'\n",
      "}\n",
      "\n",
      "item {\n",
      "    id: 2\n",
      "    name: '50 speed limit'\n",
      "}\n",
      "\n",
      "item {\n",
      "    id: 3\n",
      "    name: 'green light'\n",
      "}\n",
      "\n",
      "item {\n",
      "    id: 4\n",
      "    name: 'person'\n",
      "}\n",
      "\n",
      "item {\n",
      "    id: 5\n",
      "    name: 'red light'\n",
      "}\n",
      "\n",
      "item {\n",
      "    id: 6\n",
      "    name: 'stop sign'\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!cat {label_map_pbtxt_fname}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:32.000840Z",
     "start_time": "2021-03-14T02:16:31.949843Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GH0MEEanocn6",
    "outputId": "0d533816-a9ac-4ceb-e396-33f6dbd26394"
   },
   "outputs": [],
   "source": [
    "# look for num_classes: 6, since we have 5 different road signs and 1 person type (total of 6 types) \n",
    "!cat {pipeline_fname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "23TECXvNezIF"
   },
   "source": [
    "## Run Tensorboard(Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0H2PZs-mSCmO",
    "outputId": "805b6e5f-7aeb-4a57-ab4f-fb80f07d965f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-14 00:49:50--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
      "Resolving bin.equinox.io (bin.equinox.io)... 34.192.67.182, 3.223.68.239, 52.204.244.158, ...\n",
      "Connecting to bin.equinox.io (bin.equinox.io)|34.192.67.182|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13773305 (13M) [application/octet-stream]\n",
      "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
      "\n",
      "ngrok-stable-linux- 100%[===================>]  13.13M  39.6MB/s    in 0.3s    \n",
      "\n",
      "2021-03-14 00:49:51 (39.6 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
      "\n",
      "Archive:  ngrok-stable-linux-amd64.zip\n",
      "  inflating: ngrok                   \n"
     ]
    }
   ],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
    "!unzip -o ngrok-stable-linux-amd64.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "G8o6r1o5SC5M"
   },
   "outputs": [],
   "source": [
    "LOG_DIR = model_dir\n",
    "get_ipython().system_raw(\n",
    "    'tensorboard --logdir \"{}\" --host 0.0.0.0 --port 6006 &'\n",
    "    .format(LOG_DIR)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Ge1OX7gcSC7S"
   },
   "outputs": [],
   "source": [
    "get_ipython().system_raw('./ngrok http 6006 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5GSGxZNh8rp"
   },
   "source": [
    "### Get Tensorboard link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rjhPT9iPSJ6T",
    "outputId": "151a94bc-0ab7-4729-eb2e-5c4ca23e0076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://6e4df21479a7.ngrok.io\n"
     ]
    }
   ],
   "source": [
    "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDddx2rPfex9"
   },
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZc0RFcUjTa-"
   },
   "source": [
    "Now all inputs are set up, just train the model.   This process may take a few hours.   Since we are saving the model training results (model.ckpt-* files) in our google drive (a persistent storage that will survice the restart of our colab VM instance), we can safely leave and return a few hours later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:41.675903Z",
     "start_time": "2021-03-14T02:16:41.671901Z"
    }
   },
   "outputs": [],
   "source": [
    "model_dir = 'C:/Users/utilisateur/Desktop/object_detection/transfert_learning'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:44.734906Z",
     "start_time": "2021-03-14T02:16:44.732903Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install tensorflow-object-detection-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:16:47.595426Z",
     "start_time": "2021-03-14T02:16:47.592425Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/utilisateur/Desktop/object_detection/content/models/research/slim/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-14T02:36:08.448570Z",
     "start_time": "2021-03-14T02:36:04.153021Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CjDHjhKQofT5",
    "outputId": "3e365ddf-4535-4079-a6b3-72320d4f060b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-03-14 03:36:04.458688: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'cudart64_100.dll'; dlerror: cudart64_100.dll not found\n",
      "2021-03-14 03:36:04.459124: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:/Users/utilisateur/Desktop/object_detection/models/research/object_detection/model_main.py\", line 108, in <module>\n",
      "    tf.app.run()\n",
      "  File \"C:\\Users\\utilisateur\\Anaconda3\\envs\\train_traffic-detection\\lib\\site-packages\\tensorflow_core\\python\\platform\\app.py\", line 40, in run\n",
      "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
      "  File \"C:\\Users\\utilisateur\\Anaconda3\\envs\\train_traffic-detection\\lib\\site-packages\\absl\\app.py\", line 303, in run\n",
      "    _run_main(main, args)\n",
      "  File \"C:\\Users\\utilisateur\\Anaconda3\\envs\\train_traffic-detection\\lib\\site-packages\\absl\\app.py\", line 251, in _run_main\n",
      "    sys.exit(main(argv))\n",
      "  File \"C:/Users/utilisateur/Desktop/object_detection/models/research/object_detection/model_main.py\", line 70, in main\n",
      "    FLAGS.sample_1_of_n_eval_on_train_examples))\n",
      "TypeError: create_estimator_and_inputs() missing 1 required positional argument: 'hparams'\n"
     ]
    }
   ],
   "source": [
    "num_steps = 2000\n",
    "\n",
    "!python C:/Users/utilisateur/Desktop/object_detection/models/research/object_detection/model_main.py \\\n",
    "    --pipeline_config_path={pipeline_fname} \\\n",
    "    --model_dir='{model_dir}' \\\n",
    "    --alsologtostderr \\\n",
    "    --num_train_steps={num_steps} \\\n",
    "    --num_eval_steps={num_eval_steps}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KP-tUdtnRybs",
    "outputId": "83d04219-efef-452a-e1f6-f4f6c8703a5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 233308\n",
      "-rw------- 1 root root 58556037 Mar 14 00:52 events.out.tfevents.1615683139.5150e40e81e2\n",
      "-rw------- 1 root root 30134202 Mar 14 00:55 graph.pbtxt\n",
      "-rw------- 1 root root 58556037 Mar 14 00:55 events.out.tfevents.1615683307.5150e40e81e2\n",
      "-rw------- 1 root root    68804 Mar 14 00:55 model.ckpt-0.index\n",
      "-rw------- 1 root root 75237296 Mar 14 00:55 model.ckpt-0.data-00000-of-00001\n",
      "-rw------- 1 root root       81 Mar 14 00:55 checkpoint\n",
      "-rw------- 1 root root 16352363 Mar 14 00:55 model.ckpt-0.meta\n"
     ]
    }
   ],
   "source": [
    "!ls -ltra '{model_dir}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4Kzh3_JLVW-"
   },
   "source": [
    "# Section 5: Save and Convert Model Output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmSESMetj1sa"
   },
   "source": [
    "## Exporting a Trained Inference Graph\n",
    "Once your training job is complete, you need to extract the newly trained inference graph, which will be later used to perform the object detection. This can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "DHoP90pUyKSq"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "output_directory = '%s/fine_tuned_model' % model_dir\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikck2kvh_wTB",
    "outputId": "60a06d75-0664-4d02-8f90-0cefc617acb2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/model.ckpt-0\n"
     ]
    }
   ],
   "source": [
    "lst = os.listdir(model_dir)\n",
    "# find the last model checkpoint file, i.e. model.ckpt-1000.meta\n",
    "lst = [l for l in lst if 'model.ckpt-' in l and '.meta' in l]\n",
    "steps=np.array([int(re.findall('\\d+', l)[0]) for l in lst])\n",
    "last_model = lst[steps.argmax()].replace('.meta', '')\n",
    "\n",
    "last_model_path = os.path.join(model_dir, last_model)\n",
    "print(last_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hlxqSTTgHMHO",
    "outputId": "c16a4770-8a68-43a0-9d5d-44939df24350"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creates the frozen inference graph in fine_tune_model\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0314 00:58:17.979454 139653321807744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:20.861460 139653321807744 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:20.911336 139653321807744 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:20.959870 139653321807744 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:21.007288 139653321807744 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:21.060063 139653321807744 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:21.117106 139653321807744 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/core/post_processing.py:601: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0314 00:58:21.446394 139653321807744 deprecation.py:323] From /content/models/research/object_detection/core/post_processing.py:601: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "W0314 00:58:22.064182 139653321807744 deprecation.py:323] From /content/models/research/object_detection/exporter.py:474: get_or_create_global_step (from tf_slim.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please switch to tf.train.get_or_create_global_step\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\n",
      "I0314 00:58:23.962211 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\n",
      "I0314 00:58:23.962674 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\n",
      "I0314 00:58:23.963056 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\n",
      "I0314 00:58:23.963252 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\n",
      "I0314 00:58:23.963531 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\n",
      "I0314 00:58:23.963723 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\n",
      "I0314 00:58:23.964010 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\n",
      "I0314 00:58:23.964204 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\n",
      "I0314 00:58:23.964482 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\n",
      "I0314 00:58:23.964672 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\n",
      "I0314 00:58:23.964943 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\n",
      "I0314 00:58:23.965130 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\n",
      "I0314 00:58:23.965404 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\n",
      "I0314 00:58:23.965586 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\n",
      "I0314 00:58:23.965867 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\n",
      "I0314 00:58:23.966056 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\n",
      "I0314 00:58:23.966331 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\n",
      "I0314 00:58:23.966504 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\n",
      "I0314 00:58:23.966778 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\n",
      "I0314 00:58:23.966955 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\n",
      "I0314 00:58:23.967231 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\n",
      "I0314 00:58:23.967411 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\n",
      "I0314 00:58:23.967685 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\n",
      "I0314 00:58:23.967859 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\n",
      "I0314 00:58:23.968146 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\n",
      "I0314 00:58:23.968329 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\n",
      "I0314 00:58:23.968591 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\n",
      "I0314 00:58:23.968803 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\n",
      "I0314 00:58:23.969093 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\n",
      "I0314 00:58:23.969269 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\n",
      "I0314 00:58:23.969541 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\n",
      "I0314 00:58:23.969726 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\n",
      "I0314 00:58:23.969997 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\n",
      "I0314 00:58:23.970173 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\n",
      "I0314 00:58:23.970448 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\n",
      "I0314 00:58:23.970613 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\n",
      "I0314 00:58:23.970790 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\n",
      "I0314 00:58:23.970951 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\n",
      "I0314 00:58:23.971121 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\n",
      "I0314 00:58:23.971292 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\n",
      "I0314 00:58:23.971455 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\n",
      "I0314 00:58:23.971617 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\n",
      "I0314 00:58:23.971796 139653321807744 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
      "Instructions for updating:\n",
      "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
      "W0314 00:58:23.975599 139653321807744 deprecation.py:323] From /content/models/research/object_detection/exporter.py:653: print_model_analysis (from tensorflow.contrib.tfprof.model_analyzer) is deprecated and will be removed after 2018-01-01.\n",
      "Instructions for updating:\n",
      "Use `tf.profiler.profile(graph, run_meta, op_log, cmd, options)`. Build `options` with `tf.profiler.ProfileOptionBuilder`. See README.md for details\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "W0314 00:58:23.976467 139653321807744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/profiler/internal/flops_registry.py:142: tensor_shape_from_node_def_name (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.tensor_shape_from_node_def_name`\n",
      "263 ops no flops stats due to incomplete shapes.\n",
      "Parsing Inputs...\n",
      "Incomplete shape.\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              0\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   name\n",
      "-account_type_regexes       _trainable_variables\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          .*BatchNorm.*\n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     params\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "Incomplete shape.\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "param: Number of parameters (in the Variable).\n",
      "\n",
      "Profile:\n",
      "node name | # parameters\n",
      "_TFProfRoot (--/4.66m params)\n",
      "  BoxPredictor_0 (--/19.04k params)\n",
      "    BoxPredictor_0/BoxEncodingPredictor (--/6.92k params)\n",
      "      BoxPredictor_0/BoxEncodingPredictor/biases (12, 12/12 params)\n",
      "      BoxPredictor_0/BoxEncodingPredictor/weights (1x1x576x12, 6.91k/6.91k params)\n",
      "    BoxPredictor_0/ClassPredictor (--/12.12k params)\n",
      "      BoxPredictor_0/ClassPredictor/biases (21, 21/21 params)\n",
      "      BoxPredictor_0/ClassPredictor/weights (1x1x576x21, 12.10k/12.10k params)\n",
      "  BoxPredictor_1 (--/84.55k params)\n",
      "    BoxPredictor_1/BoxEncodingPredictor (--/30.74k params)\n",
      "      BoxPredictor_1/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_1/BoxEncodingPredictor/weights (1x1x1280x24, 30.72k/30.72k params)\n",
      "    BoxPredictor_1/ClassPredictor (--/53.80k params)\n",
      "      BoxPredictor_1/ClassPredictor/biases (42, 42/42 params)\n",
      "      BoxPredictor_1/ClassPredictor/weights (1x1x1280x42, 53.76k/53.76k params)\n",
      "  BoxPredictor_2 (--/33.86k params)\n",
      "    BoxPredictor_2/BoxEncodingPredictor (--/12.31k params)\n",
      "      BoxPredictor_2/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_2/BoxEncodingPredictor/weights (1x1x512x24, 12.29k/12.29k params)\n",
      "    BoxPredictor_2/ClassPredictor (--/21.55k params)\n",
      "      BoxPredictor_2/ClassPredictor/biases (42, 42/42 params)\n",
      "      BoxPredictor_2/ClassPredictor/weights (1x1x512x42, 21.50k/21.50k params)\n",
      "  BoxPredictor_3 (--/16.96k params)\n",
      "    BoxPredictor_3/BoxEncodingPredictor (--/6.17k params)\n",
      "      BoxPredictor_3/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_3/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
      "    BoxPredictor_3/ClassPredictor (--/10.79k params)\n",
      "      BoxPredictor_3/ClassPredictor/biases (42, 42/42 params)\n",
      "      BoxPredictor_3/ClassPredictor/weights (1x1x256x42, 10.75k/10.75k params)\n",
      "  BoxPredictor_4 (--/16.96k params)\n",
      "    BoxPredictor_4/BoxEncodingPredictor (--/6.17k params)\n",
      "      BoxPredictor_4/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_4/BoxEncodingPredictor/weights (1x1x256x24, 6.14k/6.14k params)\n",
      "    BoxPredictor_4/ClassPredictor (--/10.79k params)\n",
      "      BoxPredictor_4/ClassPredictor/biases (42, 42/42 params)\n",
      "      BoxPredictor_4/ClassPredictor/weights (1x1x256x42, 10.75k/10.75k params)\n",
      "  BoxPredictor_5 (--/8.51k params)\n",
      "    BoxPredictor_5/BoxEncodingPredictor (--/3.10k params)\n",
      "      BoxPredictor_5/BoxEncodingPredictor/biases (24, 24/24 params)\n",
      "      BoxPredictor_5/BoxEncodingPredictor/weights (1x1x128x24, 3.07k/3.07k params)\n",
      "    BoxPredictor_5/ClassPredictor (--/5.42k params)\n",
      "      BoxPredictor_5/ClassPredictor/biases (42, 42/42 params)\n",
      "      BoxPredictor_5/ClassPredictor/weights (1x1x128x42, 5.38k/5.38k params)\n",
      "  FeatureExtractor (--/4.48m params)\n",
      "    FeatureExtractor/MobilenetV2 (--/4.48m params)\n",
      "      FeatureExtractor/MobilenetV2/Conv (--/864 params)\n",
      "        FeatureExtractor/MobilenetV2/Conv/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/Conv/weights (3x3x3x32, 864/864 params)\n",
      "      FeatureExtractor/MobilenetV2/Conv_1 (--/409.60k params)\n",
      "        FeatureExtractor/MobilenetV2/Conv_1/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/Conv_1/weights (1x1x320x1280, 409.60k/409.60k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv (--/800 params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv/depthwise (--/288 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv/depthwise/depthwise_weights (3x3x32x1, 288/288 params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv/project (--/512 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv/project/weights (1x1x32x16, 512/512 params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_1 (--/4.70k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise (--/864 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/depthwise_weights (3x3x96x1, 864/864 params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_1/expand (--/1.54k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_1/expand/weights (1x1x16x96, 1.54k/1.54k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_1/project (--/2.30k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_1/project/weights (1x1x96x24, 2.30k/2.30k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_10 (--/64.90k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise (--/3.46k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_10/expand (--/24.58k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_10/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_10/project (--/36.86k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_10/project/weights (1x1x384x96, 36.86k/36.86k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_11 (--/115.78k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise (--/5.18k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_11/expand (--/55.30k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_11/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_11/project (--/55.30k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_11/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_12 (--/115.78k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise (--/5.18k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_12/expand (--/55.30k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_12/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_12/project (--/55.30k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_12/project/weights (1x1x576x96, 55.30k/55.30k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_13 (--/152.64k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise (--/5.18k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/depthwise_weights (3x3x576x1, 5.18k/5.18k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_13/expand (--/55.30k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_13/expand/weights (1x1x96x576, 55.30k/55.30k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_13/project (--/92.16k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_13/project/weights (1x1x576x160, 92.16k/92.16k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_14 (--/315.84k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise (--/8.64k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_14/expand (--/153.60k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_14/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_14/project (--/153.60k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_14/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_15 (--/315.84k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise (--/8.64k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_15/expand (--/153.60k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_15/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_15/project (--/153.60k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_15/project/weights (1x1x960x160, 153.60k/153.60k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_16 (--/469.44k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise (--/8.64k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/depthwise_weights (3x3x960x1, 8.64k/8.64k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_16/expand (--/153.60k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_16/expand/weights (1x1x160x960, 153.60k/153.60k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_16/project (--/307.20k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_16/project/weights (1x1x960x320, 307.20k/307.20k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_2 (--/8.21k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise (--/1.30k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_2/expand (--/3.46k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_2/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_2/project (--/3.46k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_2/project/weights (1x1x144x24, 3.46k/3.46k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_3 (--/9.36k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise (--/1.30k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/depthwise_weights (3x3x144x1, 1.30k/1.30k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_3/expand (--/3.46k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_3/expand/weights (1x1x24x144, 3.46k/3.46k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_3/project (--/4.61k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_3/project/weights (1x1x144x32, 4.61k/4.61k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_4 (--/14.02k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise (--/1.73k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_4/expand (--/6.14k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_4/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_4/project (--/6.14k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_4/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_5 (--/14.02k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise (--/1.73k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_5/expand (--/6.14k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_5/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_5/project (--/6.14k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_5/project/weights (1x1x192x32, 6.14k/6.14k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_6 (--/20.16k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise (--/1.73k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/depthwise_weights (3x3x192x1, 1.73k/1.73k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_6/expand (--/6.14k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_6/expand/weights (1x1x32x192, 6.14k/6.14k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_6/project (--/12.29k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_6/project/weights (1x1x192x64, 12.29k/12.29k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_7 (--/52.61k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise (--/3.46k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_7/expand (--/24.58k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_7/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_7/project (--/24.58k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_7/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_8 (--/52.61k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise (--/3.46k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_8/expand (--/24.58k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_8/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_8/project (--/24.58k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_8/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
      "      FeatureExtractor/MobilenetV2/expanded_conv_9 (--/52.61k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise (--/3.46k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/depthwise_weights (3x3x384x1, 3.46k/3.46k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_9/expand (--/24.58k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_9/expand/weights (1x1x64x384, 24.58k/24.58k params)\n",
      "        FeatureExtractor/MobilenetV2/expanded_conv_9/project (--/24.58k params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/BatchNorm (--/0 params)\n",
      "          FeatureExtractor/MobilenetV2/expanded_conv_9/project/weights (1x1x384x64, 24.58k/24.58k params)\n",
      "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256 (--/327.68k params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/weights (1x1x1280x256, 327.68k/327.68k params)\n",
      "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128 (--/65.54k params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/weights (1x1x512x128, 65.54k/65.54k params)\n",
      "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128 (--/32.77k params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/weights (1x1x256x128, 32.77k/32.77k params)\n",
      "      FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64 (--/16.38k params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/weights (1x1x256x64, 16.38k/16.38k params)\n",
      "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512 (--/1.18m params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/weights (3x3x256x512, 1.18m/1.18m params)\n",
      "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256 (--/294.91k params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
      "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256 (--/294.91k params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/weights (3x3x128x256, 294.91k/294.91k params)\n",
      "      FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128 (--/73.73k params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/BatchNorm (--/0 params)\n",
      "        FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/weights (3x3x64x128, 73.73k/73.73k params)\n",
      "\n",
      "======================End of Report==========================\n",
      "263 ops no flops stats due to incomplete shapes.\n",
      "Parsing Inputs...\n",
      "Incomplete shape.\n",
      "\n",
      "=========================Options=============================\n",
      "-max_depth                  10000\n",
      "-min_bytes                  0\n",
      "-min_peak_bytes             0\n",
      "-min_residual_bytes         0\n",
      "-min_output_bytes           0\n",
      "-min_micros                 0\n",
      "-min_accelerator_micros     0\n",
      "-min_cpu_micros             0\n",
      "-min_params                 0\n",
      "-min_float_ops              1\n",
      "-min_occurrence             0\n",
      "-step                       -1\n",
      "-order_by                   float_ops\n",
      "-account_type_regexes       .*\n",
      "-start_name_regexes         .*\n",
      "-trim_name_regexes          .*BatchNorm.*,.*Initializer.*,.*Regularizer.*,.*BiasAdd.*\n",
      "-show_name_regexes          .*\n",
      "-hide_name_regexes          \n",
      "-account_displayed_op_only  true\n",
      "-select                     float_ops\n",
      "-output                     stdout:\n",
      "\n",
      "==================Model Analysis Report======================\n",
      "Incomplete shape.\n",
      "\n",
      "Doc:\n",
      "scope: The nodes in the model graph are organized by their names, which is hierarchical like filesystem.\n",
      "flops: Number of float operations. Note: Please read the implementation for the math behind it.\n",
      "\n",
      "Profile:\n",
      "node name | # float_ops\n",
      "_TFProfRoot (--/4.49m flops)\n",
      "  FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/mul_fold (1.18m/1.18m flops)\n",
      "  FeatureExtractor/MobilenetV2/Conv_1/mul_fold (409.60k/409.60k flops)\n",
      "  FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/mul_fold (327.68k/327.68k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_16/project/mul_fold (307.20k/307.20k flops)\n",
      "  FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/mul_fold (294.91k/294.91k flops)\n",
      "  FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/mul_fold (294.91k/294.91k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_16/expand/mul_fold (153.60k/153.60k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_15/project/mul_fold (153.60k/153.60k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_15/expand/mul_fold (153.60k/153.60k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_14/project/mul_fold (153.60k/153.60k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_14/expand/mul_fold (153.60k/153.60k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_13/project/mul_fold (92.16k/92.16k flops)\n",
      "  FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/mul_fold (73.73k/73.73k flops)\n",
      "  FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/mul_fold (65.54k/65.54k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_11/expand/mul_fold (55.30k/55.30k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_11/project/mul_fold (55.30k/55.30k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_12/expand/mul_fold (55.30k/55.30k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_12/project/mul_fold (55.30k/55.30k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_13/expand/mul_fold (55.30k/55.30k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_10/project/mul_fold (36.86k/36.86k flops)\n",
      "  FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/mul_fold (32.77k/32.77k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_9/expand/mul_fold (24.58k/24.58k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_9/project/mul_fold (24.58k/24.58k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_8/project/mul_fold (24.58k/24.58k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_8/expand/mul_fold (24.58k/24.58k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_7/project/mul_fold (24.58k/24.58k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_7/expand/mul_fold (24.58k/24.58k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_10/expand/mul_fold (24.58k/24.58k flops)\n",
      "  FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/mul_fold (16.38k/16.38k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_6/project/mul_fold (12.29k/12.29k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/mul_fold (8.64k/8.64k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/mul_fold (8.64k/8.64k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/mul_fold (8.64k/8.64k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_4/expand/mul_fold (6.14k/6.14k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_6/expand/mul_fold (6.14k/6.14k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_5/project/mul_fold (6.14k/6.14k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_5/expand/mul_fold (6.14k/6.14k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_4/project/mul_fold (6.14k/6.14k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/mul_fold (5.18k/5.18k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/mul_fold (5.18k/5.18k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/mul_fold (5.18k/5.18k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_3/project/mul_fold (4.61k/4.61k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/mul_fold (3.46k/3.46k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/mul_fold (3.46k/3.46k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_3/expand/mul_fold (3.46k/3.46k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/mul_fold (3.46k/3.46k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_2/project/mul_fold (3.46k/3.46k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_2/expand/mul_fold (3.46k/3.46k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/mul_fold (3.46k/3.46k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_1/project/mul_fold (2.30k/2.30k flops)\n",
      "  MultipleGridAnchorGenerator/mul_19 (2.17k/2.17k flops)\n",
      "  MultipleGridAnchorGenerator/mul_20 (2.17k/2.17k flops)\n",
      "  MultipleGridAnchorGenerator/sub (2.17k/2.17k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/mul_fold (1.73k/1.73k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/mul_fold (1.73k/1.73k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/mul_fold (1.73k/1.73k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_1/expand/mul_fold (1.54k/1.54k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/mul_fold (1.30k/1.30k flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/mul_fold (1.30k/1.30k flops)\n",
      "  MultipleGridAnchorGenerator/sub_1 (1.20k/1.20k flops)\n",
      "  MultipleGridAnchorGenerator/mul_27 (1.20k/1.20k flops)\n",
      "  MultipleGridAnchorGenerator/mul_28 (1.20k/1.20k flops)\n",
      "  MultipleGridAnchorGenerator/mul_21 (1.08k/1.08k flops)\n",
      "  FeatureExtractor/MobilenetV2/Conv/mul_fold (864/864 flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/mul_fold (864/864 flops)\n",
      "  MultipleGridAnchorGenerator/mul_29 (600/600 flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv/project/mul_fold (512/512 flops)\n",
      "  MultipleGridAnchorGenerator/mul_35 (300/300 flops)\n",
      "  MultipleGridAnchorGenerator/mul_36 (300/300 flops)\n",
      "  MultipleGridAnchorGenerator/sub_2 (300/300 flops)\n",
      "  FeatureExtractor/MobilenetV2/expanded_conv/depthwise/mul_fold (288/288 flops)\n",
      "  MultipleGridAnchorGenerator/mul_37 (150/150 flops)\n",
      "  MultipleGridAnchorGenerator/mul_44 (108/108 flops)\n",
      "  MultipleGridAnchorGenerator/mul_43 (108/108 flops)\n",
      "  MultipleGridAnchorGenerator/sub_3 (108/108 flops)\n",
      "  MultipleGridAnchorGenerator/mul_45 (54/54 flops)\n",
      "  MultipleGridAnchorGenerator/mul_51 (48/48 flops)\n",
      "  MultipleGridAnchorGenerator/mul_52 (48/48 flops)\n",
      "  MultipleGridAnchorGenerator/sub_4 (48/48 flops)\n",
      "  MultipleGridAnchorGenerator/mul_53 (24/24 flops)\n",
      "  MultipleGridAnchorGenerator/mul_18 (19/19 flops)\n",
      "  MultipleGridAnchorGenerator/mul_17 (19/19 flops)\n",
      "  MultipleGridAnchorGenerator/sub_5 (12/12 flops)\n",
      "  MultipleGridAnchorGenerator/mul_59 (12/12 flops)\n",
      "  MultipleGridAnchorGenerator/mul_60 (12/12 flops)\n",
      "  MultipleGridAnchorGenerator/mul_26 (10/10 flops)\n",
      "  MultipleGridAnchorGenerator/mul_25 (10/10 flops)\n",
      "  MultipleGridAnchorGenerator/mul_61 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_19 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_48 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_46 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_47 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_18 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_17 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_16 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_15 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_54 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_55 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_56 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_40 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_39 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_38 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_22 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_32 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_31 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_30 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_23 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_24 (6/6 flops)\n",
      "  MultipleGridAnchorGenerator/mul_34 (5/5 flops)\n",
      "  MultipleGridAnchorGenerator/mul_33 (5/5 flops)\n",
      "  MultipleGridAnchorGenerator/mul_41 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_14 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_15 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_16 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_14 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_42 (3/3 flops)\n",
      "  MultipleGridAnchorGenerator/mul_50 (2/2 flops)\n",
      "  MultipleGridAnchorGenerator/mul_49 (2/2 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_6 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_5 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_7 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField_1/Equal (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_4 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_3 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_2 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_8 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_9 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_8 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_7 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_6 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_5 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_4 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_3 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_2 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_11 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_10 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/sub (1/1 flops)\n",
      "  Preprocessor/map/while/Less_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_18 (1/1 flops)\n",
      "  Preprocessor/map/while/Less (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/ones/Less (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_9 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_8 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_7 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_6 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_5 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_4 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_3 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_2 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_19 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/Greater_9 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_17 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_16 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_15 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_14 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_13 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_12 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_11 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_10 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/PadOrClipBoxList/sub (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_12 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/assert_equal_1/Equal (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_1 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_10 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_13 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_12 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_11 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_10 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_1 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_11 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/Minimum (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_13 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_9 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_8 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_7 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_2 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_6 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_58 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_57 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_3 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_5 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/mul_4 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_6 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_5 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_4 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_3 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_2 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Minimum (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/Greater (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/truediv (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/ChangeCoordinateFrame/sub_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/MultiClassNonMaxSuppression/SortByField/Equal (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less_1 (1/1 flops)\n",
      "  Postprocessor/BatchMultiClassNonMaxSuppression/map/while/Less (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_9 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_8 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_7 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_6 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_5 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_4 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_3 (1/1 flops)\n",
      "  MultipleGridAnchorGenerator/truediv_2 (1/1 flops)\n",
      "\n",
      "======================End of Report==========================\n",
      "2021-03-14 00:58:27.449716: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-03-14 00:58:27.521201: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-03-14 00:58:27.521304: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5150e40e81e2): /proc/driver/nvidia/version does not exist\n",
      "2021-03-14 00:58:27.521915: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-14 00:58:27.538143: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "2021-03-14 00:58:27.538604: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56346c1cbd40 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-03-14 00:58:27.538682: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/model.ckpt-0\n",
      "I0314 00:58:27.547185 139653321807744 saver.py:1284] Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/model.ckpt-0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0314 00:58:30.421986 139653321807744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/model.ckpt-0\n",
      "I0314 00:58:31.555809 139653321807744 saver.py:1284] Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/model.ckpt-0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0314 00:58:32.892467 139653321807744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "W0314 00:58:32.892807 139653321807744 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 632 variables.\n",
      "I0314 00:58:33.588305 139653321807744 graph_util_impl.py:334] Froze 632 variables.\n",
      "INFO:tensorflow:Converted 632 variables to const ops.\n",
      "I0314 00:58:33.709463 139653321807744 graph_util_impl.py:394] Converted 632 variables to const ops.\n",
      "WARNING:tensorflow:From /content/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "W0314 00:58:34.820426 139653321807744 deprecation.py:323] From /content/models/research/object_detection/exporter.py:384: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:No assets to save.\n",
      "I0314 00:58:34.821365 139653321807744 builder_impl.py:640] No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "I0314 00:58:34.821517 139653321807744 builder_impl.py:460] No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/fine_tuned_model/saved_model/saved_model.pb\n",
      "I0314 00:58:35.310415 139653321807744 builder_impl.py:425] SavedModel written to: /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/fine_tuned_model/saved_model/saved_model.pb\n",
      "INFO:tensorflow:Writing pipeline config file to /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/fine_tuned_model/pipeline.config\n",
      "I0314 00:58:35.349684 139653321807744 config_util.py:254] Writing pipeline config file to /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/fine_tuned_model/pipeline.config\n"
     ]
    }
   ],
   "source": [
    "!echo creates the frozen inference graph in fine_tune_model\n",
    "# there is an \"Incomplete shape\" message.  but we can safely ignore that. \n",
    "!python /content/models/research/object_detection/export_inference_graph.py \\\n",
    "    --input_type=image_tensor \\\n",
    "    --pipeline_config_path={pipeline_fname} \\\n",
    "    --output_directory='{output_directory}' \\\n",
    "    --trained_checkpoint_prefix='{last_model_path}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kwsBovsWZV_S",
    "outputId": "cebfbfb4-354a-4923-9a19-2cb98b4433bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "W0314 00:58:46.624244 140533115529088 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tf_slim/layers/layers.py:1089: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:49.355005 140533115529088 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:49.398619 140533115529088 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:49.437049 140533115529088 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:49.473856 140533115529088 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:49.511535 140533115529088 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "INFO:tensorflow:depth of additional conv before box predictor: 0\n",
      "I0314 00:58:49.550672 140533115529088 convolutional_box_predictor.py:156] depth of additional conv before box predictor: 0\n",
      "2021-03-14 00:58:49.605704: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-03-14 00:58:49.617031: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-03-14 00:58:49.617089: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5150e40e81e2): /proc/driver/nvidia/version does not exist\n",
      "2021-03-14 00:58:49.617507: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-14 00:58:49.623129: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "2021-03-14 00:58:49.623388: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x563b4296f640 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-03-14 00:58:49.623429: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\n",
      "I0314 00:58:51.698197 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\n",
      "I0314 00:58:51.698647 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\n",
      "I0314 00:58:51.698961 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\n",
      "I0314 00:58:51.699149 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_1/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\n",
      "I0314 00:58:51.699425 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\n",
      "I0314 00:58:51.699618 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_2/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\n",
      "I0314 00:58:51.699920 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\n",
      "I0314 00:58:51.700100 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_3/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\n",
      "I0314 00:58:51.700375 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\n",
      "I0314 00:58:51.700550 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_4/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\n",
      "I0314 00:58:51.700857 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\n",
      "I0314 00:58:51.701046 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_5/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\n",
      "I0314 00:58:51.701317 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\n",
      "I0314 00:58:51.701497 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_6/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\n",
      "I0314 00:58:51.701797 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\n",
      "I0314 00:58:51.701987 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_7/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\n",
      "I0314 00:58:51.702258 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\n",
      "I0314 00:58:51.702431 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_8/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\n",
      "I0314 00:58:51.702721 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\n",
      "I0314 00:58:51.702907 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_9/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\n",
      "I0314 00:58:51.703176 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\n",
      "I0314 00:58:51.703356 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_10/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\n",
      "I0314 00:58:51.703651 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\n",
      "I0314 00:58:51.703838 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_11/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\n",
      "I0314 00:58:51.704108 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\n",
      "I0314 00:58:51.704283 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_12/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\n",
      "I0314 00:58:51.704549 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\n",
      "I0314 00:58:51.704744 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_13/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\n",
      "I0314 00:58:51.705016 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\n",
      "I0314 00:58:51.705190 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_14/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\n",
      "I0314 00:58:51.705458 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\n",
      "I0314 00:58:51.705651 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_15/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\n",
      "I0314 00:58:51.705930 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/expand/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\n",
      "I0314 00:58:51.706107 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/expanded_conv_16/depthwise/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\n",
      "I0314 00:58:51.706379 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/Conv_1/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\n",
      "I0314 00:58:51.706547 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_2_1x1_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\n",
      "I0314 00:58:51.706728 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_2_3x3_s2_512/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\n",
      "I0314 00:58:51.706893 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_3_1x1_128/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\n",
      "I0314 00:58:51.707056 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_3_3x3_s2_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\n",
      "I0314 00:58:51.707217 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_4_1x1_128/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\n",
      "I0314 00:58:51.707378 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_4_3x3_s2_256/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\n",
      "I0314 00:58:51.707539 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_1_Conv2d_5_1x1_64/add_fold\n",
      "INFO:tensorflow:Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\n",
      "I0314 00:58:51.707913 140533115529088 quantize.py:299] Skipping quant after FeatureExtractor/MobilenetV2/layer_19_2_Conv2d_5_3x3_s2_128/add_fold\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "W0314 00:58:52.281134 140533115529088 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:127: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/model.ckpt-0\n",
      "I0314 00:58:52.988912 140533115529088 saver.py:1284] Restoring parameters from /content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/model.ckpt-0\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0314 00:58:54.174808 140533115529088 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/tools/freeze_graph.py:233: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "W0314 00:58:54.175125 140533115529088 deprecation.py:323] From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 632 variables.\n",
      "I0314 00:58:54.757341 140533115529088 graph_util_impl.py:334] Froze 632 variables.\n",
      "INFO:tensorflow:Converted 632 variables to const ops.\n",
      "I0314 00:58:54.873021 140533115529088 graph_util_impl.py:394] Converted 632 variables to const ops.\n",
      "2021-03-14 00:58:55.013976: I tensorflow/tools/graph_transforms/transform_graph.cc:317] Applying strip_unused_nodes\n"
     ]
    }
   ],
   "source": [
    "# https://medium.com/tensorflow/training-and-serving-a-realtime-mobile-object-detector-in-30-minutes-with-cloud-tpus-b78971cf1193\n",
    "# create the tensorflow lite graph\n",
    "!python /content/models/research/object_detection/export_tflite_ssd_graph.py \\\n",
    "    --pipeline_config_path={pipeline_fname} \\\n",
    "    --trained_checkpoint_prefix='{last_model_path}' \\\n",
    "    --output_directory='{output_directory}' \\\n",
    "    --add_postprocessing_op=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uTikaJKpK6No",
    "outputId": "6e5ac0bb-0b0e-46fd-aeb2-dbaa237e9527"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING frozen graph to quantized TF Lite file...\n",
      "2021-03-14 00:59:06.887170: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-03-14 00:59:06.897545: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-03-14 00:59:06.897606: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5150e40e81e2): /proc/driver/nvidia/version does not exist\n",
      "2021-03-14 00:59:06.898039: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-14 00:59:06.903733: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "2021-03-14 00:59:06.903997: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x560fdf35aa00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-03-14 00:59:06.904031: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "!echo \"CONVERTING frozen graph to quantized TF Lite file...\"\n",
    "!tflite_convert \\\n",
    "  --output_file='{output_directory}/road_signs_quantized.tflite' \\\n",
    "  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
    "  --inference_type=QUANTIZED_UINT8 \\\n",
    "  --input_arrays='normalized_input_image_tensor' \\\n",
    "  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
    "  --mean_values=128 \\\n",
    "  --std_dev_values=128 \\\n",
    "  --input_shapes=1,300,300,3 \\\n",
    "  --change_concat_input_ranges=false \\\n",
    "  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
    "  --allow_custom_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RsZi5SHSSVur",
    "outputId": "fc23a23f-667f-45da-fa07-6fc0a9d93af4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONVERTING frozen graph to unquantized TF Lite file...\n",
      "2021-03-14 00:59:15.763315: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
      "2021-03-14 00:59:15.773714: E tensorflow/stream_executor/cuda/cuda_driver.cc:318] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2021-03-14 00:59:15.773772: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (5150e40e81e2): /proc/driver/nvidia/version does not exist\n",
      "2021-03-14 00:59:15.774214: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2021-03-14 00:59:15.780017: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n",
      "2021-03-14 00:59:15.780276: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55ca67ddca00 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2021-03-14 00:59:15.780328: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "!echo \"CONVERTING frozen graph to unquantized TF Lite file...\"\n",
    "!tflite_convert \\\n",
    "  --output_file='{output_directory}/road_signs_float.tflite' \\\n",
    "  --graph_def_file='{output_directory}/tflite_graph.pb' \\\n",
    "  --input_arrays='normalized_input_image_tensor' \\\n",
    "  --output_arrays='TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3' \\\n",
    "  --mean_values=128 \\\n",
    "  --std_dev_values=128 \\\n",
    "  --input_shapes=1,300,300,3 \\\n",
    "  --change_concat_input_ranges=false \\\n",
    "  --allow_nudging_weights_to_use_fast_gemm_kernel=true \\\n",
    "  --allow_custom_ops \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "usgBZvkz0nqD",
    "outputId": "7c543808-af8f-4543-b434-7db231625b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/fine_tuned_model\n",
      "total 135035\n",
      "-rw------- 1 root root    23543 Mar 14 00:58 model.ckpt.index\n",
      "-rw------- 1 root root 18922628 Mar 14 00:58 model.ckpt.data-00000-of-00001\n",
      "-rw------- 1 root root       77 Mar 14 00:58 checkpoint\n",
      "-rw------- 1 root root  2232276 Mar 14 00:58 model.ckpt.meta\n",
      "-rw------- 1 root root 19822860 Mar 14 00:58 frozen_inference_graph.pb\n",
      "drwx------ 3 root root     4096 Mar 14 00:58 saved_model\n",
      "-rw------- 1 root root     4192 Mar 14 00:58 pipeline.config\n",
      "-rw------- 1 root root 54385448 Mar 14 00:58 tflite_graph.pbtxt\n",
      "-rw------- 1 root root 19408134 Mar 14 00:58 tflite_graph.pb\n",
      "-rw------- 1 root root  4739304 Mar 14 00:59 road_signs_quantized.tflite\n",
      "-rw------- 1 root root 18730060 Mar 14 00:59 road_signs_float.tflite\n"
     ]
    }
   ],
   "source": [
    "print(output_directory)\n",
    "!ls -ltra '{output_directory}'\n",
    "#pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\") # this is main one\n",
    "pb_fname = os.path.join(os.path.abspath(output_directory), \"frozen_inference_graph.pb\")  # this is tflite graph\n",
    "!cp '{label_map_pbtxt_fname}' '{output_directory}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz1gX19GlVW7"
   },
   "source": [
    "## Run inference test\n",
    "Test with images in repository `object_detection/data/images/test` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pzj9A4e5mj5l",
    "outputId": "c0ba215e-4bf8-42b7-e7e4-4d892801d547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/gdrive/My Drive/Colab Notebooks/TransfertLearning/training/fine_tuned_model/frozen_inference_graph.pb\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to frozen detection graph. This is the actual model that is used for the object detection.\n",
    "PATH_TO_CKPT = pb_fname\n",
    "print(PATH_TO_CKPT)\n",
    "\n",
    "# List of the strings that is used to add correct label for each box.\n",
    "PATH_TO_LABELS = label_map_pbtxt_fname\n",
    "\n",
    "# If you want to test the code with your images, just add images files to the PATH_TO_TEST_IMAGES_DIR.\n",
    "PATH_TO_TEST_IMAGES_DIR =  os.path.join(\"/content/DeepPiCar/models/object_detection/data/images/test\")\n",
    "\n",
    "assert os.path.isfile(pb_fname)\n",
    "assert os.path.isfile(PATH_TO_LABELS)\n",
    "TEST_IMAGE_PATHS = glob.glob(os.path.join(PATH_TO_TEST_IMAGES_DIR, \"*.jpg\"))\n",
    "# assert len(TEST_IMAGE_PATHS) > 0, 'No image found in `{}`.'.format(PATH_TO_TEST_IMAGES_DIR)\n",
    "print(TEST_IMAGE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CG5YUMdg1Po7",
    "outputId": "e6b4ed46-cfa6-45d0-c706-39d56aa269e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/models/research/object_detection\n"
     ]
    }
   ],
   "source": [
    "%cd /content/models/research/object_detection\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import six.moves.urllib as urllib\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "\n",
    "from collections import defaultdict\n",
    "from io import StringIO\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# This is needed since the notebook is stored in the object_detection folder.\n",
    "sys.path.append(\"..\")\n",
    "from object_detection.utils import ops as utils_ops\n",
    "\n",
    "\n",
    "# This is needed to display the images.\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from object_detection.utils import label_map_util\n",
    "\n",
    "from object_detection.utils import visualization_utils as vis_util\n",
    "\n",
    "\n",
    "detection_graph = tf.Graph()\n",
    "with detection_graph.as_default():\n",
    "    od_graph_def = tf.GraphDef()\n",
    "    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n",
    "        serialized_graph = fid.read()\n",
    "        od_graph_def.ParseFromString(serialized_graph)\n",
    "        tf.import_graph_def(od_graph_def, name='')\n",
    "\n",
    "\n",
    "label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\n",
    "categories = label_map_util.convert_label_map_to_categories(\n",
    "    label_map, max_num_classes=num_classes, use_display_name=True)\n",
    "category_index = label_map_util.create_category_index(categories)\n",
    "\n",
    "\n",
    "def load_image_into_numpy_array(image):\n",
    "    (im_width, im_height) = image.size\n",
    "    return np.array(image.getdata()).reshape(\n",
    "        (im_height, im_width, 3)).astype(np.uint8)\n",
    "\n",
    "# Size, in inches, of the output images.\n",
    "IMAGE_SIZE = (12, 8)\n",
    "\n",
    "\n",
    "def run_inference_for_single_image(image, graph):\n",
    "    with graph.as_default():\n",
    "        with tf.Session() as sess:\n",
    "            # Get handles to input and output tensors\n",
    "            ops = tf.get_default_graph().get_operations()\n",
    "            all_tensor_names = {\n",
    "                output.name for op in ops for output in op.outputs}\n",
    "            tensor_dict = {}\n",
    "            for key in [\n",
    "                'num_detections', 'detection_boxes', 'detection_scores',\n",
    "                'detection_classes', 'detection_masks'\n",
    "            ]:\n",
    "                tensor_name = key + ':0'\n",
    "                if tensor_name in all_tensor_names:\n",
    "                    tensor_dict[key] = tf.get_default_graph().get_tensor_by_name(\n",
    "                        tensor_name)\n",
    "            if 'detection_masks' in tensor_dict:\n",
    "                # The following processing is only for single image\n",
    "                detection_boxes = tf.squeeze(\n",
    "                    tensor_dict['detection_boxes'], [0])\n",
    "                detection_masks = tf.squeeze(\n",
    "                    tensor_dict['detection_masks'], [0])\n",
    "                # Reframe is required to translate mask from box coordinates to image coordinates and fit the image size.\n",
    "                real_num_detection = tf.cast(\n",
    "                    tensor_dict['num_detections'][0], tf.int32)\n",
    "                detection_boxes = tf.slice(detection_boxes, [0, 0], [\n",
    "                                           real_num_detection, -1])\n",
    "                detection_masks = tf.slice(detection_masks, [0, 0, 0], [\n",
    "                                           real_num_detection, -1, -1])\n",
    "                detection_masks_reframed = utils_ops.reframe_box_masks_to_image_masks(\n",
    "                    detection_masks, detection_boxes, image.shape[0], image.shape[1])\n",
    "                detection_masks_reframed = tf.cast(\n",
    "                    tf.greater(detection_masks_reframed, 0.5), tf.uint8)\n",
    "                # Follow the convention by adding back the batch dimension\n",
    "                tensor_dict['detection_masks'] = tf.expand_dims(\n",
    "                    detection_masks_reframed, 0)\n",
    "            image_tensor = tf.get_default_graph().get_tensor_by_name('image_tensor:0')\n",
    "\n",
    "            # Run inference\n",
    "            output_dict = sess.run(tensor_dict,\n",
    "                                   feed_dict={image_tensor: np.expand_dims(image, 0)})\n",
    "\n",
    "            # all outputs are float32 numpy arrays, so convert types as appropriate\n",
    "            output_dict['num_detections'] = int(\n",
    "                output_dict['num_detections'][0])\n",
    "            output_dict['detection_classes'] = output_dict[\n",
    "                'detection_classes'][0].astype(np.uint8)\n",
    "            output_dict['detection_boxes'] = output_dict['detection_boxes'][0]\n",
    "            output_dict['detection_scores'] = output_dict['detection_scores'][0]\n",
    "            if 'detection_masks' in output_dict:\n",
    "                output_dict['detection_masks'] = output_dict['detection_masks'][0]\n",
    "    return output_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEmFi1DFGBQ8",
    "outputId": "6bb40270-94d6-4260-ba05-e5fe72bb6b06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inferences on []\n"
     ]
    }
   ],
   "source": [
    "# running inferences.  This should show images with bounding boxes\n",
    "%matplotlib inline\n",
    "\n",
    "print('Running inferences on %s' % TEST_IMAGE_PATHS)\n",
    "for image_path in TEST_IMAGE_PATHS:\n",
    "    image = Image.open(image_path)\n",
    "    # the array based representation of the image will be used later in order to prepare the\n",
    "    # result image with boxes and labels on it.\n",
    "    image_np = load_image_into_numpy_array(image)\n",
    "    # Expand dimensions since the model expects images to have shape: [1, None, None, 3]\n",
    "    image_np_expanded = np.expand_dims(image_np, axis=0)\n",
    "    # Actual detection.\n",
    "    output_dict = run_inference_for_single_image(image_np, detection_graph)\n",
    "    # Visualization of the results of a detection.\n",
    "    vis_util.visualize_boxes_and_labels_on_image_array(\n",
    "        image_np,\n",
    "        output_dict['detection_boxes'],\n",
    "        output_dict['detection_classes'],\n",
    "        output_dict['detection_scores'],\n",
    "        category_index,\n",
    "        instance_masks=output_dict.get('detection_masks'),\n",
    "        use_normalized_coordinates=True,\n",
    "        line_thickness=2)\n",
    "    plt.figure(figsize=IMAGE_SIZE)\n",
    "    plt.imshow(image_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBbsZRWt0h6l"
   },
   "source": [
    "## Convert to Edge TPU's tflite Format  \n",
    "The only known way, at time of writing (April 2019), is to download the below quantized tflite file from above, and use [Google's web compiler](https://coral.withgoogle.com/web-compiler/) to convert to Edge TPU's tflite format.   Unfortunately, this step has to be done by hand, and NOT via a script.  \n",
    "\n",
    "Here are the requirements of Edge TPU web compiler.  If you have followed the above steps closely, you have met these requirements.\n",
    "\n",
    "- Tensor parameters are quantized (8-bit fixed-point numbers). You must use quantization-aware training (post-training quantization is not supported).   (this is why we are using `ssd_mobilenet_v2_quantized` base model and not the  `ssd_mobilenet_v2` base model   \n",
    "- Tensor sizes are constant at compile-time (no dynamic sizes).\n",
    "- Model parameters (such as bias tensors) are constant at compile-time.\n",
    "- Tensors are either 1-, 2-, or 3-dimensional. If a tensor has more than 3 dimensions, then only the 3 innermost dimensions may have a size greater than 1.\n",
    "- The model uses only the operations supported by the Edge TPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "1HhQ9y_Jcrfa",
    "outputId": "69745b3b-6a1c-4528-8be2-5d864d29dd6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw------- 1 root root 4793504 Apr 16 23:43 '/content/gdrive/My Drive/Colab Notebooks/TransferLearning/Training/fine_tuned_model/road_signs_quantized.tflite'\n"
     ]
    }
   ],
   "source": [
    "# download this file from google drive.\n",
    "!ls -lt '/content/gdrive/My Drive/Colab Notebooks/TransferLearning/Training/fine_tuned_model/road_signs_quantized.tflite'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8WzRVnDj0jt9"
   },
   "source": [
    "Wait for about 1-2 minutes for compilation to finish.  And we can download the model file as `road_signs_quantized_edgetpu.tflite`.  This is the file you need to copy to raspberry pi with TPU to run object detection.\n",
    "\n",
    "We are all done with colab notebook training, now time to switch back to raspberry pi, and run `~/DeepPiCar/models/object_detection/code/object_detection_usb.py`.  You should see a video feed where road sign and persons are boxed with confidence level around them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vG852pD0Buaq"
   },
   "source": [
    "```bash\n",
    "# make sure the the road_signs_quantized_edgetpu.tflite is in the right directory in your pi\n",
    "pi@raspberrypi:~/DeepPiCar/models/object_detection/data/model_result $ ls -ltr\n",
    "total 10040\n",
    "-rw-r--r-- 1 pi pi      97 Apr 15 01:01 road_sign_labels.txt\n",
    "-rw-r--r-- 1 pi pi 4793504 Apr 16 15:49 road_signs_quantized.tflite\n",
    "-rw-r--r-- 1 pi pi 5478080 Apr 16 15:49 road_signs_quantized_edgetpu.tflite\n",
    "\n",
    "pi@raspberrypi:~/DeepPiCar/models/object_detection $ python3 code/object_detection_usb.py\n",
    "\n",
    "------\n",
    "2019-04-16 16:22:28.489224: 13.49 FPS, 74.12ms total, 70.84ms in tf \n",
    "Green Traffic Light, 80% [[240.61578751 131.68985367]\n",
    " [287.21975327 195.79172134]] 60.42ms\n",
    "Stop Sign, 44% [[  0.         305.1651001 ]\n",
    " [180.84949493 409.32563782]] 60.42ms\n",
    "\n",
    "------\n",
    "2019-04-16 16:22:28.618309: 14.83 FPS, 67.44ms total, 60.42ms in tf \n",
    "Person, 89% [[505.6583786  279.52325821]\n",
    " [530.85933685 360.0169754 ]] 62.54ms\n",
    "Green Traffic Light, 72% [[237.96649933 130.58757782]\n",
    " [283.52127075 203.24180603]] 62.54ms\n",
    "Red Traffic Light, 62% [[283.23583603 169.27398682]\n",
    " [330.91316223 269.20692444]] 62.54ms\n",
    "Stop Sign, 56% [[ 51.01628304 165.80377579]\n",
    " [101.48646355 227.05183029]] 62.54ms\n",
    "Person, 44% [[396.8661499  298.65327835]\n",
    " [468.4034729  422.04421997]] 62.54ms\n",
    "------\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QqSFdHQvElks"
   },
   "source": [
    "# Section 6: Last Words on this Project\n",
    "\n",
    "Of course, depending many factors, not all objects in the video frame will be identified.  This is the chance to improve your model.   Try to train longer, or train with more labeled images, or augment your existing images with different zooms/rotations/contrast/lighting.  In my case, I started with one camera, which was somewhat fuzzy, and precision low.  When I switched to a HD camera, the model precision was significantly better.   This was just another way.\n",
    "\n",
    "It is awesome that for just over $100 in hardware, we can do real time object detection at home.   Moreover, thanks to Google, all you need is a browser to train this huge model!  Having fun with your own raspberry pi object detection projects!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eswamFiJLrnM"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de tensorflow_traffic_sign_detection.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
